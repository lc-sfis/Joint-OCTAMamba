# filename: MultiTaskOCTAMamba_FARGO_Interactive.py (æœ€ç»ˆç‰ˆ)
import torch
import torch.nn as nn
from typing import Dict, List, Tuple

# å¯¼å…¥å·²ç»å­˜åœ¨çš„ã€æ€§èƒ½éªŒè¯è¿‡çš„å•ä»»åŠ¡ç½‘ç»œ
try:
    from .RVMamba import    RVMamba as RV_Model
    from .FAZMamba import FAZMamba as FAZ_Model
except ImportError:
    from our_model.RVMamba import RVMamba as RV_Model
    from our_model.FAZMamba import  FAZMamba as FAZ_Model
#torch.autograd.set_detect_anomaly(True)
# --- è¾…åŠ©å‡½æ•°ï¼Œç”¨äºŽä¸­å¿ƒè£å‰ª ---
def center_crop_tensor(tensor: torch.Tensor, crop_size: Tuple[int, int]) -> torch.Tensor:
    """å¯¹(B, C, H, W)çš„å¼ é‡è¿›è¡Œä¸­å¿ƒè£å‰ª"""
    _, _, h, w = tensor.shape
    th, tw = crop_size
    x1 = int(round((w - tw) / 2.))
    y1 = int(round((h - th) / 2.))
    return tensor[:, :, y1:y1 + th, x1:x1 + tw]

class JointOCTAMamba(nn.Module):
    """
    [æ ¸å¿ƒé€»è¾‘è°ƒæ•´]:
    - `forward`æ–¹æ³•çŽ°åœ¨åŒºåˆ†è®­ç»ƒå’Œè¯„ä¼°æ¨¡å¼ã€‚
    - è®­ç»ƒæ—¶(`self.training=True`): FAZè¿”å›žè£å‰ªåŽçš„é¢„æµ‹ `faz_cropped`ã€‚
    - è¯„ä¼°æ—¶(`self.training=False`): FAZè¿”å›žä¸€ä¸ªå®Œæ•´çš„ã€å°†é¢„æµ‹ç²˜è´´å›žä¸­å¿ƒçš„å…¨å°ºå¯¸å›¾ `faz`ã€‚
    """
    def __init__(self, tasks: List[str], use_checkpoint: bool = True, faz_crop_size: int = 128, end_to_end: bool = False):
        super().__init__()
        print("ðŸš€ Initializing FARGO-style INTERACTIVE model with Train/Eval FAZ logic.")
        
        # self.tasks = tasks
        # self.faz_crop_size = (faz_crop_size, faz_crop_size)

        # self.rv_model = RV_Model()
        # self.faz_model = FAZ_Model()
        
        # # ... (åŠ¨æ€è°ƒæ•´FAZè¾“å…¥å±‚çš„ä»£ç ä¿æŒä¸å˜) ...
        # original_conv = self.faz_model.qseme.init_conv[0]
        # new_in_channels = 2
        self.tasks = tasks
        self.faz_crop_size = (faz_crop_size, faz_crop_size)
        self.end_to_end = end_to_end # [æ ¸å¿ƒ] æ–°å¢žæŽ§åˆ¶å‚æ•°
        
        mode = "END-TO-END" if self.end_to_end else "DETACHED"
        print(f"ðŸš€ Initializing Model 1 (End-to-End Control): Mode set to [ {mode} ]")
        self.rv_model = RV_Model()
        self.faz_model = FAZ_Model()
        
        # --- åŠ¨æ€è°ƒæ•´FAZè¾“å…¥å±‚ (ä¿æŒä¸å˜) ---
        original_conv = self.faz_model.qseme.init_conv[0]
        new_in_channels = 2
        if original_conv.in_channels != new_in_channels:
            new_conv = nn.Conv2d(
                in_channels=new_in_channels, out_channels=original_conv.out_channels,
                kernel_size=original_conv.kernel_size, stride=original_conv.stride,
                padding=original_conv.padding, bias=(original_conv.bias is not None)
            )
            with torch.no_grad():
                original_weights = original_conv.weight.clone()
                new_conv.weight.zero_()
                new_conv.weight[:, 0:1, :, :] = original_weights
                if original_conv.bias is not None:
                    new_conv.bias.data.copy_(original_conv.bias.data)
            self.faz_model.qseme.init_conv[0] = new_conv
            print(f"   - FAZ model's input layer modified for {new_in_channels} channels.")
        # ... (æ£€æŸ¥ç‚¹ä»£ç ä¿æŒä¸å˜) ...

    # def forward(self, x: torch.Tensor, task: str = "OCTA500_3M") -> Dict[str, torch.Tensor]:
    #     # --- RV åˆ†æ”¯ (è¡Œä¸ºå§‹ç»ˆä¸€è‡´) ---
    #     rv_output = self.rv_model(x)
    #      # --- [æ ¸å¿ƒä¿®æ”¹] æ ¹æ® self.end_to_end å†³å®šæ˜¯å¦ detach ---
    #     if self.end_to_end:
    #         # å…è®¸æ¢¯åº¦ä»Ž FAZ loss æµå›ž RV_Model
    #         rv_for_faz_input = rv_output
    #     else:
    #         # é˜»æ–­æ¢¯åº¦ (åŽŸå§‹è¡Œä¸º)
    #         rv_for_faz_input = rv_output.detach()
    #     # --- FAZ åˆ†æ”¯ (è¡Œä¸ºåŒºåˆ†è®­ç»ƒå’Œè¯„ä¼°) ---
    #     #rv_output_detached = rv_output.detach()
    #     faz_global_input = torch.cat([x, rv_for_faz_input], dim=1)
        
    #     faz_cropped_input = center_crop_tensor(faz_global_input, self.faz_crop_size)
    #     faz_cropped_output = self.faz_model(faz_cropped_input)
    #     if self.training:
    #         return {"rv": rv_output, "faz_cropped": faz_cropped_output}
    #     else:
    #         _, _, h, w = x.shape; th, tw = self.faz_crop_size
    #         x1 = int(round((w - tw) / 2.)); y1 = int(round((h - th) / 2.))
    #         faz_full_output = torch.zeros_like(x)
    #         faz_full_output[:, :, y1:y1 + th, x1:x1 + tw] = faz_cropped_output
    #         return {"rv": rv_output, "faz": faz_full_output}
    def forward(self, x: torch.Tensor, task: str = "OCTA500_3M") -> Dict[str, torch.Tensor]:
        rv_output = self.rv_model(x)
        if self.end_to_end:
            rv_for_faz_input = rv_output
        else:
            rv_for_faz_input = rv_output.detach()
        # --- é˜²æ­¢å…¨0/1è¾“å…¥å¸¦å´© ---
        rv_for_faz_input = rv_for_faz_input.clamp(0.01, 0.99)
        faz_global_input = torch.cat([x, rv_for_faz_input], dim=1)
        faz_cropped_input = center_crop_tensor(faz_global_input, self.faz_crop_size)
        faz_cropped_output = self.faz_model(faz_cropped_input)
        if self.training:
            return {"rv": rv_output, "faz_cropped": faz_cropped_output}
        else:
            _, _, h, w = x.shape; th, tw = self.faz_crop_size
            x1 = int(round((w - tw) / 2.)); y1 = int(round((h - th) / 2.))
            faz_full_output = torch.zeros_like(x[:, :1, :, :])
            faz_full_output[:, :, y1:y1 + th, x1:x1 + tw] = faz_cropped_output
            return {"rv": rv_output, "faz": faz_full_output}





