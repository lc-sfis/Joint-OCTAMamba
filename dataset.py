# filename: dataset.py (å·²æ›´æ–°ä»¥æ”¯æŒå¤šæ•°æ®é›†)

import os
import cv2
import torch
import numpy as np
import albumentations as alb
from torch.utils.data import Dataset
from typing import List, Dict, Tuple

# [MODIFIED] åŸºç¡€æ•°æ®ç›®å½•ï¼Œè€Œä¸æ˜¯ç‰¹å®šæ•°æ®é›†çš„ç›®å½•
DATA_ROOT_BASE = "dataset"
# [NEW] å®šä¹‰å·²çŸ¥çš„æ•°æ®é›†åç§°
KNOWN_DATASETS = ["OCTA500_3M", "OCTA500_6M"]

def prepareDatasets() -> Dict[str, Dict[str, Dataset]]:
    """
    [MODIFIED] å·¥åŽ‚å‡½æ•°ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶åˆ›å»ºæ‰€æœ‰å·²çŸ¥æ•°æ®é›†çš„å®žä¾‹ã€‚
    å®ƒä¼šæŸ¥æ‰¾ "dataset/OCTA500_3M" å’Œ "dataset/OCTA_6M"ã€‚
    """
    all_datasets = {}
    print("--- æ­£åœ¨å‡†å¤‡æ•°æ®é›† ---")
    for name in KNOWN_DATASETS:
        dataset_path = os.path.join(DATA_ROOT_BASE, name)
        if not os.path.exists(dataset_path):
            print(f"ðŸ” æœªæ‰¾åˆ°æ•°æ®é›† '{name}' çš„è·¯å¾„: {dataset_path}, å·²è·³è¿‡ã€‚")
            continue

        print(f"âœ… æˆåŠŸæ‰¾åˆ°æ•°æ®é›†: '{name}'")
        splits = {}
        for split in ["train", "val", "test"]:
            split_path = os.path.join(dataset_path, split)
            if os.path.exists(split_path):
                is_training = (split == "train")
                splits[split] = SegmentationDataset(split_path, is_training=is_training)
        
        if splits:
            all_datasets[name] = splits
            
    if not all_datasets:
        raise FileNotFoundError("é”™è¯¯ï¼šåœ¨ 'dataset/' ç›®å½•ä¸‹æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ•°æ®é›† ('OCTA500_3M' æˆ– 'OCTA_6M')ã€‚")
        
    return all_datasets

class SegmentationDataset(Dataset):
    """
    ä¸ºå¤šä»»åŠ¡åˆ†å‰²å‡†å¤‡æ•°æ®ã€‚æ­¤ç±»æ— éœ€ä¿®æ”¹ï¼Œå› ä¸ºå®ƒæœ¬èº«å°±æ˜¯é€šç”¨çš„ã€‚
    """
    def __init__(self, path: str, *, is_training: bool):
        super().__init__()
        self.is_training = is_training
        self.items: List[Dict[str, str]] = []
        
        image_dir = os.path.join(path, "image")
        rv_dir = os.path.join(path, "label_RV")
        faz_dir = os.path.join(path, "label_FAZ")

        for f_name in sorted(os.listdir(image_dir)):
            img_path = os.path.join(image_dir, f_name)
            if os.path.exists(img_path):
                self.items.append({
                    "name": f_name,
                    "img": img_path,
                    "rv": os.path.join(rv_dir, f_name),
                    "faz": os.path.join(faz_dir, f_name)
                })

        if self.is_training:
            p = 0.2
            self.aug = alb.Compose([
                alb.RandomBrightnessContrast(p=p),
                alb.CLAHE(p=p),
                alb.Rotate(limit=15, p=p),
                alb.VerticalFlip(p=p),
                alb.HorizontalFlip(p=p),
                alb.PiecewiseAffine(p=p),
            ], additional_targets={"mask_rv": "mask", "mask_faz": "mask"})
        else:
            self.aug = None

    def __len__(self) -> int:
        return len(self.items)

    def __getitem__(self, idx: int) -> Tuple[str, torch.Tensor, Dict[str, torch.Tensor]]:
        item = self.items[idx]
        
        img = cv2.imread(item["img"], cv2.IMREAD_GRAYSCALE).astype("float32") / 255.0
        rv_mask = cv2.imread(item["rv"], cv2.IMREAD_GRAYSCALE).astype("float32") / 255.0
        faz_mask = cv2.imread(item["faz"], cv2.IMREAD_GRAYSCALE).astype("float32") / 255.0
        
        if self.aug:
            augmented = self.aug(image=img, mask_rv=rv_mask, mask_faz=faz_mask)
            img = augmented["image"]
            rv_mask = augmented["mask_rv"]
            faz_mask = augmented["mask_faz"]
        
        img_tensor = torch.from_numpy(img).unsqueeze(0)
        rv_tensor = torch.from_numpy(rv_mask).unsqueeze(0)
        faz_tensor = torch.from_numpy(faz_mask).unsqueeze(0)
        
        labels = {"rv": rv_tensor, "faz": faz_tensor}
        
        return item["name"], img_tensor, labels

